# Day1. LLM + Prompt Engineering

## 1. Deep Learning to LLM

### 소프트웨어 단계 비유
단계별 입력과 출력 사이 거치는 것 
1. 알고리즘
    - 입력과 출력이 보장되나, 사람이 고안할 수 있는 문제만 해결 가능
2. 머신러닝 모델
    - 큰 데이터셋을 통계적 모델로 만들어 사용
    - 비용이 크고 입출력이 보장되지 않음
3. 프롬프트
    - 프롬프트: 프로그램 + 입력 데이터
    - ex) "계산기 만들어줘", "1+2는 뭐야?"

### AI Factory
- 지각, 인식만 하던 단계에서 생성하는 단계로 진입
- 어떤 과학분야에 대해서든 신규 개발 가능(신약 개발 등)

### ML과 DL의 차이점
- ML: Feature Engineering
  - 특징(feature)을 정해서 뽑아내야 함
- DL: Representation Learning
  - 표현으로써 특징을 사용
  - 이해할 수 있는 부분은 아니고, 제일 유리한 지점

### 파라미터
- 데이터 간의 관계를 통해 발견한 지식
- ex) 1달러는 1000원이다 -> 환율은 1000이다(파라미터)

### word embedding
- 컴퓨터가 언어를 처리하기 위해 의미 있는 숫자(벡터)로 바꾸는 것

### machine translation
- 인코더, 디코더로 구성
- 인코더에서 입력을 받아 히든 스테이트를 만들고, 디코더에서 이를 이용하여 출력
- 디코더에서는 인코더의 마지막 히든 스테이트만 사용함

### machine translation with attention
- 인코더, 디코더, 어텐션
- 디코더에서 인코더의 모든 히든 스테이트를 받음
- 히든 스테이트 중에서 어떤 걸 주의 깊게 볼지 정함 (어텐션)
- 어떤 스테이트가 중요한지는 딥러닝으로 학습함
  - 데이터를 충분히 부어서 딥러닝으로 어텐션 값을 학습

### transformer
- self attention, parallel processing, positional encoding 

### self attention
- 문장 내 특정 단어와 주변 단어와의 연관성을 embedding에 담음
- 자기 자신에 대한 어텐션
- 유클리드 거리보다는 코사인 유사도로 계산(차원이 늘어날수록 연산량이 적음)
- key, query, value로 구성
- key, value는 python의 dictionary 그대로
- query는 `key['query'] = value`
- 특정 단어에 대해 임베딩 실행
- 해당 임베딩의 query, key, value 값을 테이블 통해 얻음
  - 테이블은 딥러닝으로 학습
- 특정 쿼리와 유사한 key에 가중치를 주고, 모두 합해서 각 key별 어떤 걸 중점으로 볼지 계산
  - ex) 'name' query -> 'short_name' key: 가중치 약간 높음, 'name' key: 높음, 'location': 낮음
- query와 key는 관계를 비교하여 선택, 실제 계산에는 value를 사용
- in-context learning: 문장 내 문맥에 따라 값이 달라질 수 있음(LLM 특징)

### multi-head attention
- 단어에 어떤 걸 집중할지에 따라 어텐션 값이 다름(상태, 이름 등)

### parallel processing
- 문장 내 단어의 순서에 따라 의미가 달라짐
- 이런 순서의 의미를 담는 값(positional encoding)을 사용해서 계산에 사용

### Residual connection
- 어텐션하기 전 값도 사용

### cross attention
- 인코더, 디코더 사이의 어텐션
- self-attention: 인코더 또는 디코더 내 어텐션

### GPT
- 인코더, 디코더를 96개나 쌓음
- GPT는 디코더만 쌓음
  - 모델 크기를 키우니 인코더가 없어도 괜찮았음
- GPT 1부터 3까지는 구조가 동일, 사이즈만 키움
- 최근 모델들은 디코더 온리가 많음 (경험적으로 성능이 더 좋음)

### token
- 모델이 언어를 처리하는 기본 단위
- 초기에는 영어와 다른 언어 간 토큰 개수가 차이가 났지만, 최근에는 차이가 줄어들고 있음
  - 영어가 토큰 개수가 적음

### GPT3 파라미터의 구성 (총 1750억 개)
- embedding 파라미터
  - 5만개 토큰 * 12000개 차원 = 약 6억 개
- 성능 우선인지, 비용 절감 우선인지 인지하고 테크닉을 보기
- low rank: 12000*12000 -> (12000*128)*(128*12000)로 나누어 행렬 곱
  - 개수가 줄어들음
- 사실 관계는 MLP(Multi Layer Perceptron)에 저장됨

### chat-gpt와 gpt3의 차이점
- RLHF(Reinforcement Learning from Human Feedback) 사용
  - 강화학습 일종
- gpt3까지는 다음 단어의 예측만 가능
- chat-gpt는 기존 예측 대신 챗(상호작용)에 대한 강화학습을 통해 만들어짐
  - 사람의 선호(휴먼 피드백)에 따라 질문에 대한 답변을 내놓음

### gpt-4o 구조
- 워드 임베딩 뿐 아니라 문장 임베딩, 이미지 임베딩 등을 추가
- 멀티모달은 이미지, 영상, 오디오, 텍스트 등 모두 임베딩을 통해 변환 후 사용
  - 중심부는 LLM으로, 입력에는 미디어별 커넥터를 달아놓음

### gpt-o1은?
- gpt-4o까지는 수정할 기회가 없어 그대로 계속 진행
- o1 추론 모델부터는 수정할 기회를 주어 자기의 답변에 대해 생각해보고 수정 후 답변
- 마지막 한번만 하는 게 아니라 매 턴마다 자기 답변에 대해 추론 및 수정
- 추론 모델은 어떠한 질문의 정답에 대한 리워드가 아니라, 다음 단계로 가기 위해 좋은 결과물인지에 대한 리워드를 통해 학습

### 모델 고르는 법
- 리더보드 1등이 제일 나음
- 큰 모델부터 사용하고, 작은 모델로 가기(안되는 것들이 있으므로 일단 사용 후 최적화하기)
- 발전하면서 동일 성능에 대한 가격이 굉장히 빨리 내려감
  - 성능 유지만 하고 모델만 바꿔도 비용이 획기적으로 줄어들음
- 모델의 context는 프롬프트 컨텍스트 유지에 대한 크기를 나타냄
  - 128k context 제공은 책 한 권 정도
- 지금은 gemini가 제일 가격 괜찮음
- output이 input보다 비쌈
  - input은 병렬처리가 되나 output은 불가해서 더 비쌈

### 공개된 모델들의 특징
- 모델 크기를 줄이고, 학습 데이터와 학습량을 늘림
  - 추론할 때는 모델 크기만 영향이 있으므로

### GPU 성능
- 연산, 메모리 크기, 메모리 대역폭
  - 보통 성능 병목은 대역폭에서 일어남(연산이나 메모리 크기보다는 대역폭이 중요)
    - 그래서 HBM을 사용

### 최적화
- KV Caching
  - 아웃풋이 끝날 때까지 key, value를 캐싱해서 씀
  - 대역폭과 시간이 병목이라면 사용
- batching
  - 메모리에 불러오는 게 비용이 크고 연산은 비용이 크지 않음
  - 한번 불러오고 나서 연산은 모두 모아 한번에 진행?
- MoE(Mixture of Experts)
  - 큰 모델 하나를 여러 개의 모델로 나누어서 필요한 모듈 모델만 사용
  - DeepSeek는 여기에 MTP(multi token prediction)으로 여러 개 토큰 단위로 예측
- Flash Attention
  - 기존 CUDA를 쓰는 대신 더 최적화하여 사용

### 모델 평가
- latency: 첫번쨰 토큰이 나오는 데에 걸리는 시간
  - 연산 속도가 큰 영향
- throughput: 첫번째 이후의 처리 토큰 개수
  - 대역폭이 큰 영향

### on device AI
- 휴대폰 등 개인 장치 등에 모델을 올려 사용
- 상대적으로 작은 모델을 사용하여 성능이 아직 좋진 않고, 장치 스펙도 영향
- 클라우드 AI보다 비용 및 속도 면에서는 훨씬 좋음
  - 데이터 초개인화 및 최적화도 가능
- 애플, 삼성 갤럭시에서는 on device, cloud 둘 다 사용 중
- 양자화(quantization)
  - 32-bit 표현을 16-bit로 바꾸는 등으로 모델 크기를 줄임
  - 일반적으로는 4-bit가 가장 가성비가 좋지만 최적화 비용이 커서 현재 8-bit를 사용
  - 대역폭, 크기에서는 유리하지만 연산 속도는 더 느려짐

## 2. Prompt Engineering
- 모든 상황에 유리한 건 없음 (있다면 이미 모델에 반영됨)
- 언어학적인 방식보다는 모델별 공식 가이드나 모델의 구조 및 학습 과정에서 사용된 방법을 사용하기
- 모델이 발전하면서 불필요한 테크닉이 많아지고 있음

### softmax & temperature
- temperature가 작을수록 max 함수에 가까워지고, 클수록 1/n에 가까워짐
  - chat-gpt의 답변이 매번 다른 이유는 temperature 때문

### few shot
- 지시와 함께 예시를 주기
- zero-shot, one-shot, few-shot 등 예시 개수에 따라 이름이 다름

### CoT Chain of Thought
- 일반 모델을 추론 모델처럼 동작하게 지시하는 것 (단계별로 풀도록 지시)
- 추론 모델은 필요 없음

### Self Consistency
- 같은 질문에 대한 답변을 여러 개 만들고 그중 가장 많이 나온 답을 선택
- 모델 성능이 좋아지면서 중요성이 낮아짐

### Least to Most
- 분할정복으로 풀도록 지시
- CoT보다 일반적으로는 더 좋음
- 모델 성능이 좋아지면서 중요성이 낮아짐
- 여전히 유효한 부분: 분할정복 방식으로 지시, 애매하면 질문하도록
  - 현재 모델들은 시스템 프롬프트에 자체적으로 미리 가지고 있음

### X of thought
- 각 문제에 대해 최적의 자료구조 및 알고리즘 등을 사용하도록 지시
- prompt의 context는 기존 프로그래밍에서의 알고리즘과 자료구조를 대체하는 역할로 사용

### Plan and Solve
- 문제를 푸는 과정을 명시적으로 계획을 짜주면 성능이 좋아짐

### ReAct
- LLM의 가능성인 Reason + Act을 사용
  - 특정 문제에 대한 추론 후 어떤 액션(검색 등)으로 문제를 풀지에 대한 선택을 통해 답변

### Generated Knowledge Prompting
- LLM에게 자신이 알고 있는 관련 지식을 늘어놓도록 지시 후 이를 이용하여 답변하도록
- 추론 모델에 이미 반

### Table Meets LLM
- 프롬프트를 SQL로 바꿔서 데이터를 바로 가져와 사용
- 굳이 임베딩을 통해 풀기보다는 기존 데이터를 가져와 바로 사용하도록

### 프롬프트 주의사항
- 모델이 달라지는 경우 대부분 잘 동작하지 않음
- 매번 업데이트 해주어야 함